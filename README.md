# Multi-Scale Visual Prompting (MS-VP) for Image Classification

A comprehensive PyTorch research codebase for exploring Multi-Scale Visual Prompting across multiple architectures and datasets.

## ğŸ¯ Overview

This project implements **Multi-Scale Visual Prompting**, a parameter-efficient method for enhancing image classification models by learning visual prompts at multiple scales (global, mid-level, and local).

**Key Features:**
- ğŸ”¬ Three backbone architectures: CNN, ResNet18, ViT-Tiny
- ğŸ“Š Three benchmark datasets: MNIST, FashionMNIST, CIFAR-10
- ğŸ¨ Multiple fusion strategies: Addition, Concatenation, Gated
- ğŸ” Comprehensive ablation studies
- ğŸ“ˆ Publication-quality visualizations (GradCAM, prompts, metrics)
- âš¡ Multi-GPU training support

## ğŸ“ Project Structure

```
msvp/
â”œâ”€â”€ datasets/               # Dataset loaders
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ loaders.py         # MNIST, FashionMNIST, CIFAR-10
â”œâ”€â”€ models/                # Model architectures
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ prompting.py       # Multi-Scale Prompting module
â”‚   â”œâ”€â”€ cnn.py            # Baseline CNN
â”‚   â”œâ”€â”€ resnet.py         # ResNet18 with MS-VP
â”‚   â””â”€â”€ vit.py            # ViT-Tiny with MS-VP
â”œâ”€â”€ utils/                 # Utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py         # Configuration management
â”‚   â”œâ”€â”€ training.py       # Training loops
â”‚   â”œâ”€â”€ metrics.py        # Evaluation metrics
â”‚   â””â”€â”€ visualization.py  # Plotting and GradCAM
â”œâ”€â”€ experiments/           # Experiment outputs
â”‚   â”œâ”€â”€ checkpoints/      # Model checkpoints
â”‚   â”œâ”€â”€ logs/             # Training logs
â”‚   â””â”€â”€ plots/            # Visualizations
â”œâ”€â”€ train.py              # Main training script
â”œâ”€â”€ ablation_prompt_scale.py   # Ablation: prompt scales
â”œâ”€â”€ ablation_fusion.py         # Ablation: fusion strategies
â”œâ”€â”€ ablation_backbone.py       # Ablation: backbones
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### Installation

```bash
cd ./msvp
pip install -r requirements.txt
```

### Basic Training

Train ResNet18 on CIFAR-10 with Multi-Scale Prompting:

```bash
python train.py \
  --dataset cifar10 \
  --model resnet18 \
  --use_prompt \
  --epochs 10 \
  --batch_size 128 \
  --multi_gpu
```

Train baseline (without prompting):

```bash
python train.py \
  --dataset cifar10 \
  --model resnet18 \
  --epochs 10 \
  --batch_size 128 \
  --multi_gpu
```

### Available Options

**Datasets:** `mnist`, `fashion`, `cifar10`  
**Models:** `cnn`, `resnet18`, `vit`  
**Fusion Types:** `add`, `concat`, `gated`  
**Prompt Scales:** `global`, `global+mid`, `full`

### Multi-GPU Training

The codebase automatically uses all available GPUs when `--multi_gpu` is specified:

```bash
CUDA_VISIBLE_DEVICES=0,1,2 python train.py \
  --dataset cifar10 \
  --model resnet18 \
  --use_prompt \
  --multi_gpu
```

## ğŸ”¬ Ablation Studies

### 1. Prompt Scale Ablation

Compare global-only, global+mid, and full multi-scale prompting:

```bash
python ablation_prompt_scale.py \
  --dataset cifar10 \
  --model resnet18 \
  --epochs 10
```

**Output:** 
- Results table comparing configurations
- Bar plot: `experiments/plots/ablation_prompt_scale.png`
- CSV: `experiments/plots/ablation_prompt_scale_results.csv`

### 2. Fusion Strategy Ablation

Compare addition, concatenation, and gated fusion:

```bash
python ablation_fusion.py \
  --dataset cifar10 \
  --model resnet18 \
  --epochs 10
```

**Output:** 
- Comparison of fusion strategies
- Plots: `experiments/plots/ablation_fusion.png`
- CSV: `experiments/plots/ablation_fusion_results.csv`

### 3. Backbone Ablation

Compare CNN, ResNet18, and ViT across all datasets:

```bash
# Run all datasets (will take longer)
python ablation_backbone.py --epochs 10

# Or specific dataset
python ablation_backbone.py --dataset cifar10 --epochs 10
```

**Output:**
- 3Ã—2 accuracy matrix (3 models Ã— baseline/prompt)
- Plots: `experiments/plots/ablation_backbone.png`
- CSV: `experiments/plots/ablation_backbone_results.csv`

## ğŸ“Š Visualization

Visualizations are automatically generated during training and saved to `experiments/plots/`.

### Training Curves

Automatically saved after training to `experiments/logs/<exp_name>/training_curves.png`

### Prompt Visualization

Visualize learned prompts from a trained model:

```python
from models import ResNet18
from utils import visualize_prompts
import torch

model = ResNet18(in_channels=3, use_prompt=True)
model.load_state_dict(torch.load('path/to/checkpoint.pth')['model_state_dict'])

prompts = model.prompting.get_prompts()
visualize_prompts(prompts, save_path='prompts_viz.png')
```

### GradCAM Heatmaps

Compare attention patterns between baseline and MS-VP models:

```python
from utils import visualize_gradcam
from models import ResNet18
import torch

model = ResNet18(in_channels=3, use_prompt=True)
model.load_state_dict(torch.load('path/to/checkpoint.pth')['model_state_dict'])
model.eval()

# Get a sample image
x = torch.randn(1, 3, 32, 32).cuda()

# Target the last conv layer
target_layer = model.layer4[-1].conv2  # For ResNet18

visualize_gradcam(
    model, x, target_layer,
    save_path='gradcam.png',
    title='GradCAM: ResNet18 with MS-VP'
)
```

## ğŸ“ˆ Expected Results

### MNIST (10 epochs)
- **Baseline CNN**: ~98-99%
- **CNN + MS-VP**: ~99%+
- **ResNet18 + MS-VP**: ~99.5%+

### CIFAR-10 (10 epochs)
- **Baseline ResNet18**: ~88-90%
- **ResNet18 + MS-VP**: ~90-92%
- **ViT-Tiny + MS-VP**: ~85-88%

*Note: Results may vary. For publication, run 3 seeds and report mean Â± std.*

## ğŸ›  Advanced Usage

### Custom Hyperparameters

```bash
python train.py \
  --dataset cifar10 \
  --model vit \
  --use_prompt \
  --fusion_type gated \
  --prompt_scales full \
  --epochs 50 \
  --batch_size 256 \
  --lr 1e-3 \
  --scheduler onecycle \
  --optimizer adamw \
  --weight_decay 5e-4 \
  --dropout 0.1 \
  --seed 42
```

### Resume Training

```python
from utils import load_checkpoint
import torch

checkpoint = load_checkpoint(
    'experiments/checkpoints/best_model.pth',
    model, optimizer, scheduler
)

start_epoch = checkpoint['epoch'] + 1
best_acc = checkpoint['best_val_acc']
```

## ğŸ”§ Configuration Files

After each training run, configuration and results are saved:

- `experiments/checkpoints/<exp_name>/config.json` - Full configuration
- `experiments/checkpoints/<exp_name>/results.json` - Final metrics
- `experiments/checkpoints/<exp_name>/best_model.pth` - Best checkpoint
- `experiments/logs/<exp_name>/training_log.json` - Training history


